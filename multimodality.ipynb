{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da882a3-ad18-4bee-bf2a-6ddbb8407920",
   "metadata": {},
   "source": [
    "Import essential libraries for neural network training, data loading, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb816c4-b9b6-46d2-886b-b0b6f1c036b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import neural network training libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import basic computation and data visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import umap.plot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "# Import the custom dataset class\n",
    "from mnist_dataset import MNISTDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d16621-cd1b-45ed-b728-4f558abfd815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ef6c64-4783-45ba-8fa7-7e558b587bdc",
   "metadata": {},
   "source": [
    "Load the MNIST dataset and apply image transformations for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247432f-80ed-459c-bb38-9c5721cf5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv\n",
    "data = pd.read_csv('digit-recognizer/train.csv')\n",
    "val_count = 1000\n",
    "\n",
    "# Define common transformations for both training and validation data\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "# Split data into training and validation datasets\n",
    "dataset = MNISTDataset(data.iloc[:-val_count], default_transform)\n",
    "val_dataset = MNISTDataset(data.iloc[-val_count:], default_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3103df1-2feb-4992-b03a-e1a87e5bb224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ab6dee-2df4-4b91-946e-8a098a63a853",
   "metadata": {},
   "source": [
    "Set up data loaders to efficiently handle the training and validation batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c84bb8-26df-4766-9b68-5f2e5438eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training data\n",
    "trainLoader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,  # Feel free to modify the batch size\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=100\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation data\n",
    "valLoader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc33ecc-9b85-404b-a745-8d8e56d41d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a89eb561-acee-4a64-b7e3-034208606ea5",
   "metadata": {},
   "source": [
    "Display examples of anchor images and corresponding contrastive images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03cdb0-3dd8-4925-848e-edc1755a62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images with labels\n",
    "def show_images(images, title=''):\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(9, 3))\n",
    "    for i in range(num_images):\n",
    "        img = np.squeeze(images[i])\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some examples from the training set\n",
    "for batch_idx, (anchor_images, contrastive_images, distances, labels) in enumerate(trainLoader):\n",
    "    # Convert tensors to numpy arrays\n",
    "    anchor_images = anchor_images.numpy()\n",
    "    contrastive_images = contrastive_images.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    # Display the first four samples\n",
    "    show_images(anchor_images[:4], title='Anchor Image')\n",
    "    show_images(contrastive_images[:4], title='+/- Example')\n",
    "    \n",
    "    break  # Display only one batch for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ba181-5ad4-4e05-bfd3-ba37385bf7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33328a0c-8bfe-4c6f-84c5-5afe5f80d53f",
   "metadata": {},
   "source": [
    "Define a neural network architecture for MNIST image representation in 64 dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26224717-87ec-4c35-8c70-b9f4bf6fbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 64),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc5cfc-8237-4194-97b4-f2a5a67c7b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "740fec75-ec1e-4758-9fac-bacc714ed9c2",
   "metadata": {},
   "source": [
    "Define a custom contrastive loss function using cosine similarity for comparing anchor and contrastive images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca95e86-94d8-4c3d-965b-001bceb5c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.similarity = nn.CosineSimilarity(dim=-1, eps=1e-7)\n",
    "\n",
    "    def forward(self, anchor, contrastive, distance):\n",
    "        score = self.similarity(anchor, contrastive)  # Calculate cosine similarity\n",
    "        return nn.MSELoss()(score, distance)  # Minimize difference between calculated and ideal distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf5322-17f6-43ea-8e84-4dfadba5e2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe92859f-b38d-4217-8794-72ec97a7cfe2",
   "metadata": {},
   "source": [
    "Set up the network, optimizer, loss function, and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cef6b-47f7-4b6d-9dbe-a52fa7d70807",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    \n",
    "net = net.to(device)\n",
    "\n",
    "# Define optimizer, loss function, and learning rate scheduler\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "loss_function = ContrastiveLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71654a7-426e-476c-a6f2-5f97a6c9cede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a55e721-97b1-4825-be3b-1d02eec567be",
   "metadata": {},
   "source": [
    "Implement the training loop to train the network for a defined number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11e8d4-b836-4ce0-84f5-1707eeea7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save model checkpoints\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "def train_model(epoch_count=10):\n",
    "    net = Network()\n",
    "    lrs = []\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        epoch_loss = 0\n",
    "        batches = 0\n",
    "        print('epoch -', epoch)\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        print('learning rate', lrs[-1])\n",
    "    \n",
    "        for anchor, contrastive, distance, label in tqdm(trainLoader):\n",
    "            batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            anchor_out = net(anchor.to(device))\n",
    "            contrastive_out = net(contrastive.to(device))\n",
    "            distance = distance.to(torch.float32).to(device)\n",
    "            loss = loss_function(anchor_out, contrastive_out, distance)\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        losses.append(epoch_loss.cpu().detach().numpy() / batches)\n",
    "        scheduler.step()\n",
    "        print('epoch_loss', losses[-1])\n",
    "    \n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "        torch.save(net.state_dict(), checkpoint_path)\n",
    "\n",
    "    return {\"net\": net, \"losses\": losses}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10a74a-f8d1-441e-8b79-a1c234f251f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ca6cb9-d433-4ca1-8249-42218e9d8ee8",
   "metadata": {},
   "source": [
    "Optionally load a pre-trained model from a saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c607e-fdc0-4c05-95e0-afb023cb2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint():\n",
    "    checkpoint = torch.load('checkpoints/model_epoch_99.pt')\n",
    "    net = Network()\n",
    "    net.load_state_dict(checkpoint)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "train = False  # Set to True if you want to train the model\n",
    "if train:\n",
    "    training_result = train_model()\n",
    "    model = training_result[\"net\"]\n",
    "else:\n",
    "    model = load_model_from_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa70ec0-e0cd-4196-9249-8cef6212fbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "152240f4-69d8-492d-a20b-176cc6f84f68",
   "metadata": {},
   "source": [
    "Visualize the training loss curve after training or load the pre-trained loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d4133-c8c3-40bc-82fb-5e8eeaa99737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "if train:\n",
    "    plt.plot(training_result[\"losses\"])\n",
    "    plt.show()\n",
    "else:\n",
    "    display(Image(filename=\"images/loss-curve.png\", height=600, width=600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8ad7f-da54-4e32-a398-314615de64f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b81c4f4-7240-4a1a-8e0f-1f563746744e",
   "metadata": {},
   "source": [
    "Reduce the dimensionality of encoded training data from 64D to 3D and plot it interactively using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a4df1-8e64-4e27-81dd-2c82df04d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for anchor, _, _, label in tqdm(trainLoader):\n",
    "        output = model(anchor.to(device))\n",
    "        encoded_data.extend(output.cpu().numpy())\n",
    "        labels.extend(label.cpu().numpy())\n",
    "\n",
    "encoded_data = np.array(encoded_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Apply PCA for dimensionality reduction (64D to 3D)\n",
    "pca = PCA(n_components=3)\n",
    "encoded_data_3d = pca.fit_transform(encoded_data)\n",
    "\n",
    "# Plot in 3D\n",
    "scatter = go.Scatter3d(\n",
    "    x=encoded_data_3d[:, 0],\n",
    "    y=encoded_data_3d[:, 1],\n",
    "    z=encoded_data_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=4, color=labels, colorscale='Viridis', opacity=0.8),\n",
    "    text=labels, \n",
    "    hoverinfo='text',\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"MNIST Dataset - Encoded and PCA Reduced 3D Scatter Plot\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"PC1\"),\n",
    "        yaxis=dict(title=\"PC2\"),\n",
    "        zaxis=dict(title=\"PC3\"),\n",
    "    ),\n",
    "    width=1000, \n",
    "    height=750,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scatter], layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2692f-2967-4e48-8f34-a714e20ad9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8051c6d-1c7d-430c-ae8e-e68c10b4c292",
   "metadata": {},
   "source": [
    "Use UMAP for dimensionality reduction and visualize the data in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f93caf-4918-4aa8-8319-8d3520764a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(random_state=42, metric='cosine').fit(encoded_data)\n",
    "umap.plot.points(mapper, labels=labels)\n",
    "\n",
    "# Use Euclidean metric for UMAP visualization\n",
    "mapper = umap.UMAP(random_state=42).fit(encoded_data) \n",
    "umap.plot.points(mapper, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce22ef-cfae-434c-b94c-0a33b26ca8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68dc938-8e44-4b9e-8ebb-5fab167e0002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7adef08-c531-4164-86d4-4ef0626133e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
